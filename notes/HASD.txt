WHU-Hi-Station

100-50
97 0.837
20 0.811
17 0.846

HSI_dataset img,mask范围 配置文件推理size和padsize， max——value，overlap_infer 

0-600 100-50 0.999 no-quantile '/home/ljt21/ad/RSAD/outputs/2022-09-13/17-29-22/epochs-0/anomaly_map.tif'

600-1200 50-50  0.962 q_value1 = torch.quantile(temp_img, q=0.5) 
        q_value2 = torch.quantile(temp_img, q=0.98)  
        q_value3 = torch.quantile(temp_img, q=0.02) /home/ljt21/ad/RSAD/outputs/2022-09-13/18-17-56/epochs-0/anomaly_map.tif 

1200-2400 50-25 0.949 q_value1 = torch.quantile(temp_img, q=0.5)
        q_value2 = torch.quantile(temp_img, q=0.96)   
        q_value3 = torch.quantile(temp_img, q=0.04) ‘/home/ljt21/ad/RSAD/outputs/2022-09-13/20-36-42/epochs-0/anomaly_map.tif’

2400-3600 50-25 0.946 no-quantile '/home/ljt21/ad/RSAD/outputs/2022-09-13/20-40-24/epochs-0/anomaly_map.tif' 

最终AUC 0.93612  

目前去做直接推理
1. 证明了CAE迁移是不行的。现有深度算法结合低秩和稀疏决定了无法迁移
2. /home/ljt21/ad/RSAD/outputs/2022-09-07/21-55-35/epochs-97  5-5-0.942 20-20-0.946 50-25-0.962 50-50 0.957
   50 overlap 0.965, 但是视觉效果不好.20epoch的参数虽然可以到了0.97，但是hydice不如rxd，因此采用97epoch参数

Cri epoch17-50-25可以到了0.973

多模融合推理取结果均值效果不太好，归一化前取均值也不行，但是相乘可以。不过受到极值影响，目视效果不好

基于PiCANet做得HYDICE结果可以到了0.9960

基于BASNet做得
1 d1 0.9858 d2 0.9893 d3 0.9759 d4 0.9429

1. 之前一直效果不好的根本原因是，应该取patch，而非直接大图做 
对比bs=1，bs=4
bs=1，峰值0.980，但是只有一次，且相对不稳定
bs=4，峰值0.970，多次，相对稳定。选取bs=4

对比efficientnet-b7和resnet50 （bs=4） 
efficientnet-b7 训练不起来

对比一个洞还是max4 min2 
一个洞可以到了0.9849

对比制作模拟数据的时候加一个不加模拟信号的情况试试
不加是0.9849
加了是0.958，还是不加了

对比测试10个洞
一个洞0.9849
10个洞0.909

对比loss的mean和0.98max
mean直接到了0.6几

hard_example mining机制不太管用

加了self.pre_transform = A.Compose([
            A.ChannelShuffle(p=0.5), 
            A.RandomBrightnessContrast(p=0.5), 
        ]
训练不起来，一直在0.5左右 


取消transformer blocks从头训练做对比实验，发现transformer blocks还是加入了一定的先验，一开始就很高

发现即使取消transformer blocks，仍然可以训练起来，或许神经网络没有在学习差异，期待一下最终的AUC可以多高

为了防止神经网络没有在学习差异，img每次训练时候也变换一下试试

对比depth为3
一个windowsize为4，8，16 起步只有0.40,最终是0.71
一个windowsize为16，16，16 起步就是很高0.74 很快超越4，8，16，最终可以超越0.94

对比是否使用transformer block
一个使用 只能0.931
一个不使用，更高一些 可以达到0.95多

对比是否使用block最后的mlp，发现没什么差异
对比是否使用attention内部的mlp，发现没什么差异

对比加了fusion策略和不加
fusion后起步0.88，不加的话起步0.78.最终峰值fusion0.953，不加的峰值0.943

对比加了fusion后是否使用transformerblock
不加起步0.40，加了起步就有0.88.证明了加了fusion的trasnformer起了作用

对比是否加新做的prediction head，
加了峰值0.962，不加0.953

对比是否把fusion和prediction的kernel换成3*3，换了以后发现只能到0.83

不同block个数
1 起步0.40多，和不加transformer block类似
4 起步0.88，上升的慢，峰值0.953
8 起步0.78，但很快0.932，上升的快 峰值0.973
12 起步0.79，上升速度略慢于8 blocks，峰值0.983 
18 训练不起来了

测试一开始的conv换成1*1，
从0.962变成了0.982（4 block）,换成8个block反而下降到0.94

将changing_x中的元素换成attention_map
峰值0.9888

latent dim
1 0.962
8 0.920

loss从mean改成分位点loss = torch.quantile(loss, 0.98)
AUC到了0.993

'16, 16, 32, 32'
AUC到了0.987, 不如'16, 16, 16, 16'

loss从loss = torch.quantile(loss, 0.98)改成loss = torch.quantile(loss, 0.98) + loss.mean()
AUC到了0.984

测试
changing_x.append(x)
        for i in range(len(changing_x)-1): 
            changing_x[i] = changing_x[i] * changing_x[i+1]
            changing_x[i] = torch.sum(changing_x[i], axis=1)/torch.sqrt((torch.tensor(self.latent_dim, device=self.device)))
            changing_x[i] = changing_x[i].unsqueeze(1)
精度上到0.85就开始骤降

对比是否每轮多训练几次
发现这样会带来不稳定性

对比测试是否加上局部归一化
加了后，起步很高可以0.978，但是峰值只到了0.987。可视化发现归一化导致右下角区域更红了

对比加了模长信息
x2 = sample['image']
        x2 = torch.sum((x2**2), axis=0)
        x2 = torch.sqrt(x2)
        x2 = (x2 - x2.min())/(x2.max() - x2.min())
        x2 = x2.unsqueeze(0) 
        # x2 = x2.expand((10,x2.shape[1],x2.shape[2]))
        sample['image'] = torch.cat([x2, sample['image'], x2], axis=0)
可以稳定0.9931

网络中间a+b直接加人造的特征，会破坏掉学习的分布，效果不好。要加就加在输入上

最后是不是可以搞一个对比的预测头，使用大的卷积核得到一个，然后算内积
attention可以加一个模块，关注注意力前后差值，以及注意力前后的特征融合